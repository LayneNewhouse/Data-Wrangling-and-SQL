{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0080FF;\">Udacity - Data Analyst Nanodegree </span>\n",
    "## <span style=\"color:#B45F04;\">Project 3: Wrangle OpenStreetMap Data</span>\n",
    "### <span style=\"color:black;\">Created by: Layne Newhouse</span>\n",
    "### <span style=\"color:#6E6E6E;\">Submitted: October 4, 2016</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Background Information*\n",
    "For the OpenStreetMap project I chose to look at the Toronto Ontario Canada area, where I currently live. The following url is a link to the 79MB, uncompressed, OSM XML file for the Toronto area which is provided by MapZen.\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/metro/toronto_canada/\n",
    "\n",
    "The bulk of this project will be based on programmatically extracting and cleaning the OpenStreetMap (OSM) data around Toronto, exploring that data in order to develop a deeper understanding of what we have and what types of problems might arise, and then exporting the cleaned data into an SQL database. Once the data is in the SQL database the following questions from the rubric will be answered:\n",
    "\n",
    "- What is the size of the file?\n",
    "- What are the total number of nodes and ways in this dataset?\n",
    "- How many unique users have contributed to this OSM dataset?\n",
    "- Extract the number of nodes found for particular node types i.e. cafe, shop etc. \n",
    "- Any additional questions or statistics that might come up after exploring the dataset.\n",
    "\n",
    "After these questions have been answered I will discuss further ideas for improvements on the dataset as well as benefits and problems that could result from these improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the extracted OSM file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I begin the data wrangling process by parsing through the tags in the extracted OSM XML file. Each unique tag encountered will be added to the dictionary 'tags' with the tag name being the key and the value starting at '1'. For each addition tag encountered with the same key, the value will be increased by one until each tag has been looked at and accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 96337,\n",
      " 'nd': 5542308,\n",
      " 'node': 4873350,\n",
      " 'osm': 1,\n",
      " 'relation': 6429,\n",
      " 'tag': 4820840,\n",
      " 'way': 710944}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "\n",
    "OSMFILE = 'toronto_canada.osm'\n",
    "tag_tags = []\n",
    "\n",
    "def count_tags(filename):\n",
    "        tags = {}\n",
    "        tag = []\n",
    "        for event, elem in ET.iterparse(filename):\n",
    "            tag.append(elem.tag)\n",
    "        for i in tag:\n",
    "            if i in tags:\n",
    "                tags[i] += 1\n",
    "            else:\n",
    "                tags[i] = 1         \n",
    "        return tags\n",
    "\n",
    "def test():\n",
    "    tags = count_tags(OSMFILE)\n",
    "    pprint(tags)\n",
    "    tag_tags.append(tags['tag'])\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dictionary 'tags' as seen above highlights the different tags found in the OSM file. Nothing from this dictionary seems out of the ordinary so we will continue forward with a general idea of the number of node and way tags that we will be dealing with. The list 'tag_tags' was added and will be used in a later section of the code in order to get a better understanding of the tags tagged <'tag'>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will explore the keys associated with each 'tag' tag in order to assess the types of keys that we will be working with. We will begin by defining the different types of regular expressions that we will be looking for i.e. all lower cases, lower cases with one colon in between, problem character etc. The code will then parse through each element and look for elements tagged 'tag', then extract the 'k' value, or the key, from these tags and add them to the appropriate bin based on the re.search expression. Within this section of code, there are many commented out print statements that I chose to leave in the code because I believe that during the data exploration phase, there is no 'one code fits all' and it is important to always be altering the code to achieve a greater understanding of the dataset that you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem tags make up 2.508214% of the total 4820840 tags in the 'Toronto Canada' dataset\n",
      "115230 out of the 120917 problem tags (95%) come from geobase, canvec and tiger tags\n",
      "i.e. 'geobase:acquisitionTechnique', 'canvec:UUID', 'tiger:name_base_1'\n",
      "The remaining 5687 (0.117967%) tags can either be changed by hand or ignored;\n",
      "of the remaining problem tags 142 are unique.\n",
      "\n",
      "{'lower': 2769359,\n",
      " 'lower_2colon': 5398,\n",
      " 'lower_colon': 1924345,\n",
      " 'other': 120917,\n",
      " 'problemchars': 821}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "import re\n",
    "import operator\n",
    "\n",
    "OSMFILE = 'toronto_canada.osm'\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "lower_2colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "omit_keys = []\n",
    "\n",
    "def key_type(element, keys, other, problem_characters):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.get(\"k\")\n",
    "        if re.search(lower,k):\n",
    "            keys['lower'] += 1\n",
    "        elif re.search(lower_colon,k):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif re.search(lower_2colon,k):\n",
    "            keys['lower_2colon'] += 1\n",
    "        elif re.search(problemchars,k):\n",
    "            keys['problemchars'] += 1\n",
    "            if k in problem_characters:\n",
    "                problem_characters[k] += 1\n",
    "            else:\n",
    "                problem_characters[k] = 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "            if k in other:\n",
    "                other[k] += 1\n",
    "            else:\n",
    "                other[k] = 1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"lower_2colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    other = {}\n",
    "    problem_characters = {}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys, other, problem_characters)\n",
    "    sort_other = sorted(other.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sort_problem_characters = sorted(problem_characters.items(), \n",
    "                                     key=operator.itemgetter(1), reverse=True)\n",
    "    #The two print statements below will display problem tags and problem characters\n",
    "    #as well as the amount of times that these tag appears\n",
    "    #print \"OTHER:\", sort_other\n",
    "    #print \"OTHER:\", sort_problem_characters\n",
    "    \n",
    "    gct_count = 0\n",
    "    for key in other:\n",
    "        omit_keys.append(key)\n",
    "        if 'geobase' in key or 'canvec' in key or 'tiger' in key:\n",
    "            #print key, other[key]\n",
    "            gct_count += other[key]\n",
    "    for key in problem_characters:\n",
    "        omit_keys.append(key)\n",
    "    unique_not_gct = 0 \n",
    "    for i in omit_keys:\n",
    "        if 'geobase' not in i and 'canvec' not in i and 'tiger' not in i:\n",
    "            unique_not_gct += 1\n",
    "            #print i\n",
    "    \n",
    "    print                                                                                \n",
    "    print \"Problem tags make up %f%% of the total %d tags in the 'Toronto Canada' dataset\" % (100*float(keys['other'])/tag_tags[0], \n",
    "                                                                                               tag_tags[0])\n",
    "    print \"%d out of the %d problem tags (%d%%) come from geobase, canvec and tiger tags\" % (gct_count, keys['other'], \n",
    "                                                                                             100*float(gct_count)/keys['other'])\n",
    "    print \"i.e. 'geobase:acquisitionTechnique', 'canvec:UUID', 'tiger:name_base_1'\"\n",
    "    print \"The remaining %d (%f%%) tags can either be changed by hand or ignored;\" % (keys['other']-gct_count, \n",
    "                                                                                    100*(keys['other']-gct_count)/float(tag_tags[0]))\n",
    "    print\"of the remaining problem tags %d are unique.\" % unique_not_gct\n",
    "    print\n",
    "    #The print statment below will show all tags that are being omitted from this dataset\n",
    "    #print omit_keys\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    keys = process_map(OSMFILE)\n",
    "    pprint(keys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dictionary, as can be seen above, highlights the different compositions of tags within this database. The vast majority of tag keys fall within one of the first three types (lower case with either zero, one, or two colons) which we are ready to deal with and are excepting to see in the following data cleaning sections. 'problemchars' are keys with characters that have been noted as problematic (i.e. [=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]). For the purpose of the project, these keys will be ignored but will be addressed later in this document as an area of further improvement for the dataset. The 'other' dictionary key is filled with keys that do not fall into any of the other categories, in order to get a better understanding of these keys were I used the commented out 'sort_other' list to view all of the 'other' tags sorted by frequency of occurrence. When doing this, I found that the majority of these keys came from geobase, canvec, and tiger which seem to be names of programs that have assisted in the filling out of elements in the openStreetMaps datasets. When doing a search in a sample raw xml dataset, I found that the nodes and ways that were tagged with these types of tags always had other tags that would compliment the node or way. Thus I decided that these tags were of no value to my dataset and would be removed for clarity and cleaning purposes. \n",
    "\n",
    "Finally I have used a few print statements to provide me with some statistics regarding the problematic tags. From here I found out that these 'other' tag keys make up approximately 2.5 percent of the total tag keys but when I remove the geobase, canvec and tiger tags from this count it drops down significantly to approximately 0.1 percent or 142 unique keys. For the sake of this project, these keys will be ignored but diccussed later on in the improvements discussion section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions marks the beginning of the data cleaning phase. This function specifically parses through elements looking for tags specifically with key ('k') values of \"addr:street\" and then analyses the value ('v') associated with that tag. Once the street address 'value' is obtained, the function extracts the last word, which in most cases will be the street type, and compares these values to the excepted street types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 389 street type tags\n",
      "of these, 95 tags were updated for constistancy\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "OSMFILE = \"toronto_canada.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "updated_street_names = {}\n",
    "\n",
    "# referenced: https://en.wikipedia.org/wiki/Street_suffix for common street names\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Crescent\", \"Line\", \"Circle\", \"Gardens\", \"Close\", \"Concession\",\n",
    "            \"Gate\", \"Grove\", \"Heights\", \"Hill\", \"Path\", \"Run\", \"Sideroad\", \"Terrace\", \"Way\", \"Townline\",\n",
    "            \"North\", \"East\", \"South\", \"West\"]\n",
    "\n",
    "# 'Highway  5  West' => 'Highway 5 West' \n",
    "# 'Sideroad 5 Tosorontio' => 'Tosorontio Sideroad 5'\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"street\": \"Street\",\n",
    "            \"STREET\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"avenue\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Fernway\": \"Fern Way\",\n",
    "            \"By-pass\": \"Bypass\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Cres\": \"Crescent\",\n",
    "            \"Cresent\": \"Crescent\",\n",
    "            #\"Crest\": \"Crescent\", -> Individual cases...\n",
    "            \"Crt\": \"Court\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Cv\": \"Court\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Driver\": \"Drive\",\n",
    "            \"E\": \"East\",\n",
    "            \"E.\": \"East\",\n",
    "            \"Hrbr\": \"Harbour Way\",\n",
    "            \"Lan\": \"Lane\",\n",
    "            \"Lanes\": \"Lane\",\n",
    "            \"Ldg\": \"Landing\",\n",
    "            \"N\": \"North\",\n",
    "            \"S\": \"South\",\n",
    "            \"S.\": \"South\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"W\": \"West\",\n",
    "            \"W.\": \"West\",\n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    #re.sub(pattern, repl, string, count=0, flags=0)\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            if street_type in mapping:\n",
    "                name = re.sub(street_type, mapping[street_type], name)\n",
    "    return name\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    print \"The dataset contains %d street type tags\" % len(st_types)\n",
    "    #pprint(dict(st_types))\n",
    "    count = 0\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            if better_name != name:\n",
    "                # The print statment below shows all street names that were altered\n",
    "                #print name, \"=>\", better_name\n",
    "                count += 1\n",
    "                updated_street_names[name] = better_name\n",
    "    print \"of these, %d tags were updated for constistancy\" % count\n",
    "    # the variable updated_street_names contains a dictionary using the old names as the key \n",
    "    # and the updated names as the values\n",
    "    #print updated_street_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the function above was run for the first few times, the street names that did not show up as expected were printed out in a list. At this point, some manual labour had to be done in order to fill in the 'mapping' dictionary as can be seen in the body of the function above. This was done by observing the unexpected street types which were printed out and then adding each respective mapping to the 'mapping' dictionary so that the next time the function was run, the street names would be updated and removed from the unexpected list. The commented out '#print name, \"=>\", better_name' prints out all of the updated street types as they are found in the update_street_types dictionary. This dictionary will be used in a subsequent function in order to replace old street names with the updated street names before being added to the SQL database. As an additional note, there were some cases where suffixes such as 'Crest' could not be properly mapped due to the fact that some instances of the word were referring to just 'Crest' where others were referring to the word 'Crescent'. Cleaning of this type would have to be done in more detail if desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning cont. & Exporting to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below pulls in the Toronto Canada OSM data, parses through it, cleans some of the data based on the dictionaries and lists passed on from the previous functions, shapes the data based on the schema provided, and exports the cleaned data to .csv files which are ready to be imported into the SQL database. The create_tag_dictionary function is where all of the previous work in the data cleaning process is realized. For each element, the tag key is analyzed and checked against problem characters, the omit_keys list, and the updated_street_names dictionary. If any key shows up in one of these checks, the key and/or value is either changed appropriately or omitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"toronto_canada.osm\"\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"node_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"way_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"way_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# ================================================== #\n",
    "#               Cleaning & Shaping                   #\n",
    "# ================================================== #\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for field in NODE_FIELDS:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "        for tag in element.iter(\"tag\"):    \n",
    "            tags = create_tag_dict(element, tag, tags, default_tag_type)\n",
    "        #print {'node': node_attribs, 'node_tags': tags} ...debugging\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for field in WAY_FIELDS:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "        \n",
    "        position = 0\n",
    "        for node in element.iter(\"nd\"):\n",
    "            node_dict = {'id': element.attrib['id'], 'node_id': node.attrib['ref'], 'position': position}\n",
    "            position += 1\n",
    "            way_nodes.append(node_dict)\n",
    "            \n",
    "        for tag in element.iter(\"tag\"):    \n",
    "            tags = create_tag_dict(element, tag, tags, default_tag_type)\n",
    "        #print {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "def create_tag_dict(element, tag, tags, tag_type):\n",
    "    k = tag.attrib['k']\n",
    "    if not re.search(PROBLEMCHARS, k):\n",
    "        # The following three lines omit problem keys found earlier as well as update\n",
    "        # old street names with the improved street names from the previous script\n",
    "        if k not in omit_keys:\n",
    "            if k in updated_street_names:\n",
    "                k = updated_street_names[k]\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if \":\" not in k:\n",
    "                tag_dict['key'] = k\n",
    "            else:\n",
    "                tag_type = tag.attrib['k'].split(':',1)[0]\n",
    "                k = tag.attrib['k'].split(':',1)[1]\n",
    "                tag_dict['key'] = k\n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "            tag_dict['type'] = tag_type\n",
    "            tags.append(tag_dict)\n",
    "    return tags\n",
    "\n",
    "        \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as node_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(node_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of setting up the SQL Database in python is fairly systematic. It starts by connecting sqlite3 to the toronto_canada database filename, creating a cursor object and then executing commands to create each of the individual tables. After the schemas are added and the changes are committed, the program goes row by row through the csv files pulling them into python and then inserting them into the new SQL database by table and row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "nodes_csv = 'nodes.csv'\n",
    "node_tags_csv = 'node_tags.csv'\n",
    "ways_csv = 'ways.csv'\n",
    "way_tags_csv = 'way_tags.csv'\n",
    "way_nodes_csv = 'way_nodes.csv'\n",
    "\n",
    "# Connect to the database, if it doesn't exist it will be created\n",
    "sqlite_file = 'toronto_canada.db'\n",
    "db = sqlite3.connect(sqlite_file)\n",
    "db.text_factory = str\n",
    "# Create a cursor object\n",
    "c = db.cursor()\n",
    "\n",
    "# Create the tables\n",
    "c.execute('''\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "c.execute('''\n",
    "CREATE TABLE node_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id))\n",
    "''')\n",
    "c.execute('''\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "c.execute('''\n",
    "CREATE TABLE way_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id))\n",
    "''')\n",
    "c.execute('''\n",
    "CREATE TABLE way_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id))    \n",
    "''')\n",
    "# Commit changes\n",
    "db.commit()\n",
    "\n",
    "# Read in the data\n",
    "with open(nodes_csv, 'rb') as read_csv:\n",
    "    reader = csv.DictReader(read_csv) #comma is default delimiter\n",
    "    nodes_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in reader]\n",
    "with open(node_tags_csv, 'rb') as read_csv:\n",
    "    reader = csv.DictReader(read_csv) #comma is default delimiter\n",
    "    node_tags_db = [(i['id'], i['key'], i['value'], i['type']) for i in reader]\n",
    "with open(ways_csv, 'rb') as read_csv:\n",
    "    reader = csv.DictReader(read_csv) #comma is default delimiter\n",
    "    ways_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in reader]\n",
    "with open(way_tags_csv, 'rb') as read_csv:\n",
    "    reader = csv.DictReader(read_csv) #comma is default delimiter\n",
    "    way_tags_db = [(i['id'], i['key'], i['value'], i['type']) for i in reader]\n",
    "with open(way_nodes_csv, 'rb') as read_csv:\n",
    "    reader = csv.DictReader(read_csv) #comma is default delimiter\n",
    "    way_nodes_db = [(i['id'], i['node_id'], i['position']) for i in reader]\n",
    "    \n",
    "# Insert formatted data\n",
    "c.executemany('''\n",
    "    INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?);''', nodes_db)\n",
    "c.executemany('''\n",
    "    INSERT INTO node_tags(id, key, value, type) \n",
    "    VALUES (?, ?, ?, ?);''', node_tags_db)\n",
    "c.executemany('''\n",
    "    INSERT INTO ways(id, user, uid, version, changeset, timestamp) \n",
    "    VALUES (?, ?, ?, ?, ?, ?);''', ways_db)\n",
    "c.executemany('''\n",
    "    INSERT INTO way_tags(id, key, value, type) \n",
    "    VALUES (?, ?, ?, ?);''', way_tags_db)\n",
    "c.executemany('''\n",
    "    INSERT INTO way_nodes(id, node_id, position) \n",
    "    VALUES (?, ?, ?);''', way_nodes_db)\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following SQL queries are performed on the cleaned Toronto Canada OpenStreetMaps database (toronto_canada.db) in order to answer the initial questions posed at the beginning of this document as well as further explore the database and come up with additional statistics making use of one or more of the tables within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of this database is 681844 kB\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('PRAGMA PAGE_SIZE')\n",
    "page_size = c.fetchall()\n",
    "c.execute ('PRAGMA PAGE_COUNT')\n",
    "page_count = c.fetchall()\n",
    "db.close()\n",
    "# Page size shows the size of the pages in bytes\n",
    "# Page count shows the amount of pages that are allocated to the database\n",
    "page_size = page_size[0][0]\n",
    "page_count = page_count[0][0]\n",
    "print \"The size of this database is %d kB\" % (page_size*page_count/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above query uses the PRAGMA function to access internal (non-table) data from the database. The total size of the database is the product of the size of each page multiplied by the number of pages in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4873350 node tags and 710944 way tags combining to a total of 5584294 way/node tags.\n",
      "Of these tags, 506 are not unique between the nodes and the ways tables.\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT *\n",
    "    FROM (SELECT count(id) FROM ways\n",
    "    UNION SELECT count(id) FROM nodes)\n",
    "''')\n",
    "idWaysNodes = c.fetchall()\n",
    "c.execute ('''\n",
    "    SELECT COUNT(id)\n",
    "    FROM (SELECT id FROM ways\n",
    "    UNION SELECT id FROM nodes)\n",
    "''')\n",
    "idUnique = c.fetchall()\n",
    "db.close()\n",
    "# changing the order in which ways and nodes appear in the union \n",
    "# does not seem to change the order of the final table\n",
    "print \"There are %d node tags and %d way tags combining to a total of %d way/node tags.\" % (idWaysNodes[1][0], idWaysNodes[0][0], \n",
    "                                                                                            idWaysNodes[1][0] + idWaysNodes[0][0])\n",
    "print \"Of these tags, %d are not unique between the nodes and the ways tables.\" % (idWaysNodes[1][0] + idWaysNodes[0][0]\n",
    "                                                                                   - idUnique[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985 unique users have contributed to the node and way tags for this database\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT COUNT(DISTINCT uid)\n",
    "    FROM (SELECT uid FROM nodes\n",
    "    UNION SELECT uid FROM ways)\n",
    "''')\n",
    "all_rows = c.fetchall()\n",
    "db.close()\n",
    "unique_uid = all_rows[0][0]\n",
    "print \"%d unique users have contributed to the node and way tags for this database\" % unique_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'fast_food', 2996),\n",
      " (u'restaurant', 2744),\n",
      " (u'bench', 2352),\n",
      " (u'post_box', 1964),\n",
      " (u'cafe', 1409),\n",
      " (u'parking', 1233),\n",
      " (u'waste_basket', 1171),\n",
      " (u'bank', 1046),\n",
      " (u'fuel', 1001),\n",
      " (u'pharmacy', 743)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT value, COUNT(*)\n",
    "    FROM node_tags\n",
    "    WHERE key = 'amenity'\n",
    "    GROUP BY value\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 10\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above queries answer the initial questions presented at the beginning of the project document, where:\n",
    "\n",
    "- The size of this database is 681,844 kB\n",
    "- The total number of nodes and ways are 4,873,350 and 710,944 respectively\n",
    "- There are 1985 unique users contributing specifically to this database\n",
    "- There are 1409 Cafe nodes, 2996 Fast Food nodes and 1046 Bank nodes to name a few\n",
    "\n",
    "The following queries are a result of further investigation into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'source', 483041),\n",
      " (u'street', 472140),\n",
      " (u'housenumber', 472136),\n",
      " (u'city', 414610),\n",
      " (u'highway', 85423),\n",
      " (u'country', 50682),\n",
      " (u'name', 35732),\n",
      " (u'state', 26598),\n",
      " (u'amenity', 25980),\n",
      " (u'province', 23890)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT key, COUNT(*)\n",
    "    FROM node_tags\n",
    "    GROUP BY key\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 10\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'City of Toronto', 111651),\n",
      " (u'City of Hamilton', 40073),\n",
      " (u'Mississauga', 35113),\n",
      " (u'City of Brampton', 25557),\n",
      " (u'City of Vaughan', 16962),\n",
      " (u'Town of Markham', 16852),\n",
      " (u'Oakville', 15435),\n",
      " (u'City of Oshawa', 12031),\n",
      " (u'Richmond Hill', 11346),\n",
      " (u'City of Burlington', 11050),\n",
      " (u'Town of Whitby', 10638),\n",
      " (u'Town of Ajax', 7437),\n",
      " (u'Town of Milton', 7397),\n",
      " (u'City of Pickering', 6754),\n",
      " (u'Town of Caledon', 6701),\n",
      " (u'Town of Newmarket', 5851),\n",
      " (u'Town of Halton Hills', 5834),\n",
      " (u'Toronto', 5121),\n",
      " (u'Burlington', 4653),\n",
      " (u'Town of Whitchurch-Stouffville', 4398)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT value, COUNT(*)\n",
    "    FROM node_tags\n",
    "    WHERE key = 'city'\n",
    "    GROUP BY value\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 20\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having first looked at the top ten keys in node_tags, I had then refined my query to do a search on the top twenty 'city' tags within this table to see which cities had the highest number of tags. From here an opportunity to further clean the data became apparent as there were separate tags for 'City of Toronto' and 'Toronto' which could easily be combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'source', 464856),\n",
      " (u'highway', 242805),\n",
      " (u'interpolation', 226512),\n",
      " (u'surface', 161827),\n",
      " (u'lanes', 150670),\n",
      " (u'name', 145372),\n",
      " (u'building', 102326),\n",
      " (u'attribution', 87188),\n",
      " (u'access', 59372),\n",
      " (u'is_in', 49761)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "# Shows the top 10 most common keys within way_tags\n",
    "c.execute ('''\n",
    "    SELECT key, COUNT(*)\n",
    "    FROM way_tags\n",
    "    GROUP BY key\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 10\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545 ways have only a single node assigned to them\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "c.execute ('''\n",
    "    SELECT COUNT(*) FROM \n",
    "    (SELECT COUNT(*) as nodeCount\n",
    "    FROM way_nodes\n",
    "    GROUP BY id)\n",
    "    WHERE nodeCount = 1\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "db.close()\n",
    "print \"%d ways have only a single node assigned to them\" % all_rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out many other queries associated with the tag keys and values I also began to investigate timestamps to see what kind of information I could derive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'2016', 52011),\n",
      " (u'2015', 57476),\n",
      " (u'2014', 52523),\n",
      " (u'2013', 52881),\n",
      " (u'2012', 144792),\n",
      " (u'2011', 99937),\n",
      " (u'2010', 241083),\n",
      " (u'2009', 114439),\n",
      " (u'2008', 17638),\n",
      " (u'2007', 4653),\n",
      " (u'2006', 372)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "# Number of ways and nodes added to the database each year \n",
    "c.execute ('''\n",
    "    SELECT STRFTIME('%Y', timestamp) as year, COUNT(*)\n",
    "    FROM (SELECT timestamp FROM nodes\n",
    "    UNION SELECT timestamp FROM ways)\n",
    "    GROUP BY year\n",
    "    ORDER BY year DESC\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'2010-09', 75669),\n",
      " (u'2010-04', 59329),\n",
      " (u'2010-07', 58310),\n",
      " (u'2012-04', 33734),\n",
      " (u'2012-05', 29342)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "# Which months of which years had the most ways and nodes added to the database\n",
    "c.execute ('''\n",
    "    SELECT STRFTIME('%Y-%m', timestamp) as yearMonth, COUNT(*)\n",
    "    FROM (SELECT timestamp FROM nodes\n",
    "    UNION SELECT timestamp FROM ways)\n",
    "    GROUP BY yearMonth\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 5\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'andrewpmk', 72535),\n",
      " (u'Victor Bielawski', 2949),\n",
      " (u'bdustan', 101),\n",
      " (u'mfagan', 25),\n",
      " (u'MikeyCarter', 23),\n",
      " (u'tixuwuoz', 18),\n",
      " (u'bgibbard', 13),\n",
      " (u'brandoncote', 10),\n",
      " (u'salocinbake', 8),\n",
      " (u'emvee', 7),\n",
      " (u'Andre68', 2),\n",
      " (u'Sven L', 1),\n",
      " (u'de239', 1)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(sqlite_file)\n",
    "c = db.cursor()\n",
    "# In the year-month with the highest way/node entries, which users were adding to the database and how many\n",
    "# entries did each user contribute\n",
    "c.execute ('''\n",
    "    SELECT user, COUNT(*)\n",
    "    FROM (SELECT timestamp, user FROM nodes\n",
    "    UNION SELECT timestamp, user FROM ways),\n",
    "    (SELECT STRFTIME('%Y-%m', timestamp) as yearMonth\n",
    "    FROM (SELECT timestamp FROM nodes\n",
    "    UNION SELECT timestamp FROM ways)\n",
    "    GROUP BY yearMonth\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 1) as subq\n",
    "    WHERE STRFTIME('%Y-%m', timestamp) = yearMonth\n",
    "    GROUP BY user\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    ''')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion & Ideas for Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having run multiple queries over the full size database it was apparent that more cleaning could have been done. By looking at some of the top values in the cities key, it was evident that there were duplicate values for the same meaning, such as 'City of Toronto' and 'Toronto' as we had seen before. A full look into each of the way and node keys would allow you to further clean the data but figuring out when to stop would be difficult and would depend on the end use or reason for querying. For example, one might be more thorough if they were creating a navigation system out of this data as to not give anyone the wrong directions or misleading information.\n",
    "\n",
    "Additionally, I think it would be interesting to include more graphics (graphs and charts) regarding the time/date/year that ways and nodes are being added. On top of this, there could be a plethora of statistics regarding which users are adding data and what days or times of days they are doing this.\n",
    "\n",
    "Furthermore, there were a handful of tags that were either omitted or not fixed in the earlier stages of cleaning. A script could be written in which it tells the user how many tags or elements have not been fixed and then on a one by one basis either have the user either manually correct any unclean data or cross check some of the map data with another application such as google maps.\n",
    "\n",
    "Some issues with these improvements are that they could become incredibly time consuming and difficult to know when to stop since we are dealing with such a large set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#5F04B4;'>References </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following websites were refered to during the completion of this project.\n",
    "- stackoverflow.com\n",
    "- https://en.wikipedia.org/wiki/Street_suffix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
